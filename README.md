
## Implementation of Bloom Filters/Count-Min Sketches in Apache Spark Streaming

## Sort Description of Bloom Filters

In this project the streaming edition of the Bloom Filters is implemented. The targets is to answer queries about previous appearence of a specific IP in the past streams. The Spark Streaming Apache was utilized as it is a powerful big data tool and a great number of scientists keep it up-to-date. Although, the Spark Streaming cannot handle tuple-per-tuple processing the micro-batching approach is more than enough to develop such a simple sketch as Bloom Filter is. The main advantage of the Bloom Filters is that they need reasonable size of memory, therefore, Spark is the ideal candidate as it always caches the part of the data that is commonly retrieved. Hence, the Bloom Filter will be with high odd in-memory, something that results in high performance.
Our implementation was based on key-value pairs, due to the fact that Spark provides special operations on RDDs containing key/value pairs. These RDDs are called pair RDDs. Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key. It is common to extract fields from an RDD (representing, for instance, an event time or other identifier) and use those fields as keys in pair RDD operations.
Firstly, we developed a python code for generating IP addresses. The IP addresses will feed the streaming Bloom Filter in order to be trained or evaluated. The generated IP could be any possible combination of IPV4. Note that IPs are written in the file without dots (i.e 147563845321 is the IP 147.563.854.321). The streaming algorithm takes as input two files; one for training and the one for queries answering. Since our code is running locally (only for validation) the files divided in micro-batches which are stored in a Queue Stream instance. Queue Stream is an internal library of Apache Spark that is used commonly to simulate the streaming process. Each of the micro windows could have special duration and a pre-fixed number of data.
Secondly, we designed a graph with the operations that should be executed to keep the Bloom Filter up-to-date.. The main execution plan is illustrated in the following figure.

![fig1](https://user-images.githubusercontent.com/25617530/97561088-4bd09500-19e8-11eb-8c46-52a1ec3ed484.PNG)



As the above figure shows, the procedure starts with the mapping of each IP to array positions of the Bloom Filter by leveraging k hash functions. The produced key/value pairs are corresponding to indices of the Boom Filter (keys) that must be set to ‘1’ (values). Since the mappers are running in parallel, the results are united using “union” operator and the reducebykey() operator is called to remove the duplicates (minimize overload). Since the current RDD will be comprised of new arrivals from the input stream, the state of the bloom filter must be updated. By the end of the stream the state memory will include all the array positions that are set to ‘1’, therefore, the positions that are ‘0’ will not be existed in the state.

![fig2](https://user-images.githubusercontent.com/25617530/97561102-4e32ef00-19e8-11eb-89c9-1a7e661d405b.PNG)

Query Stream is separate and independent from the training stream (suppose two different sockets, one for training and the other for queries). Specifically, the file with queries is comprised of IP addresses. If the ith IP is in the set, the algorithm returns (print or save to HDFS) that address, otherwise the i-th address is not written in the output file. The second figure depicts the way that a query is answered. Firstly, the IPs are mapped to key/value pairs and the reducebykey() operator takes place to remove the duplicates. Secondly, the tuples are translated into array positions using k hash functions and after uniting the hash function’s results, the joining operation between the current state of the stream and the query stream returns new key/value pairs, which are formulated as <Key=Array Positions, Value= List of [ IP, 1 ]>. Last but not least, from the aforementioned tuples we keep only the list of values. The first elements of the list will be the new keys and the second the new values. Finally, the number of the different values with the same key is counted. If the number of the ith IP is equal to the number of the hash functions, all the array positions that the hash functions returned are existed in the Bloom Filter and their value is equal to ‘1’. On the other hand, if the value of the ith IP is less than the number of the hash functions, means that one of the array positions has value ‘0’. In that case, the ith IP had not appeared at the previous events.

## Sort Description of Count-Min Sketch
In computing, the count–min sketch (CM sketch) is a probabilistic data structure that serves as a frequency table of events in a stream of data. It uses hash functions to map events to frequencies, but unlike a hash table uses only sub-linear space, at the expense of overcounting some events due to collisions. Count–min sketches are essentially the same data structure as the counting Bloom filters. However, they are used differently and therefore sized differently; a count-min sketch typically has a sublinear number of cells, related to the desired approximation quality of the sketch, while a counting Bloom filter is more typically sized to match the number of elements in the set. The goal of the basic version of the count–min sketch is to consume a stream of events, one at a time, and count the frequency of the different types of events in the stream. At any time, the sketch can be queried for the frequency of a particular event type i (0 ≤ i ≤ n for some n), and will return an estimate of this frequency that is within a certain distance of the true frequency, with a certain probability P(error).
The actual sketch data structure is a two-dimensional array of w columns and d rows. Associated with each of the d rows is a separate hash function; the hash functions must be pairwise independent. The parameters w and d can be chosen by setting w = ⌈e/ε⌉ and d = ⌈ln 1/δ⌉, where the error in answering a query is within an additive factor of ε with probability 1 − δ (see below), and e is Euler's number.
When a new event of type i arrives, we update as follows; for each row j of the table, apply the corresponding hash function to obtain a column index k = hj(i). Then increment the value in row j, column k by one.
Several types of queries are possible on the stream.
• Point query
• Inner product

## Implementation of Count-Min Sketch in Spark
The implementation of that scenario will be quite same as Bloom Filters. Specifically, once can convert the w x d Count-Min Sketch to a single dimension array with size 1 x (w x d). Therefore, the operations that was utilized in Bloom Filters could be reused in that problem with slight variation. During training, the Bloom Filter saves only a bit (0/1), which is necessary to make decisions about the previous events. On the other hand, Count-Min sketch have to keep update w x d counters, hence, even though the mapping to key/value pairs using the appropriate hash functions remains, the state that the stream should save are counters instead of a bit (i.e similar with Word Count). In that case, the reducebykey() operator count all the appearances of an event during a certain micro-batch and by using the updatebykey() operator the updated sketch is saved, as it is illustrated in Figure 3. This process is repeatedly executed until the end of the stream.

![fig3](https://user-images.githubusercontent.com/25617530/97561110-50954900-19e8-11eb-9480-b5e2314ed428.PNG)
![fig4](https://user-images.githubusercontent.com/25617530/97561112-51c67600-19e8-11eb-90f9-af12ce386b8f.PNG)

Answering point queries is similar to answering point queries in Bloom Filters. The main difference stems from the fact that in the final stage the minimum value among the elements of the d hash functions should be calculated. The reducebykey() operation in the final transformation, as it is depicted in Figure 4, exposes that minimum value for each IP that is queried instead of extracting only an IP address as Bloom Filters does.


## Run Sketches 
1. Run the write_the_ip_files.py file to generate data or use the already existed ip_adresses_file.txt file
2. Open the project with Eclipse (Import also .pom file)
3. Run the Count-Min or the Bloom Filter .java files

